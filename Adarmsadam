# Program 7 – Adagrad, RMSprop, Adam using Neural Network and Visualization
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# 1️⃣ Load MNIST dataset
train = datasets.MNIST(root='./data', train=True, download=True,
                       transform=transforms.ToTensor())
test = datasets.MNIST(root='./data', train=False,
                      transform=transforms.ToTensor())

train_loader = DataLoader(train, batch_size=128, shuffle=True)
test_loader = DataLoader(test, batch_size=1000, shuffle=False)

# 2️⃣ Simple Neural Network
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(-1, 28*28)
        x = F.relu(self.fc1(x))
        return self.fc2(x)

# 3️⃣ Optimizers to compare
optimizers = {
    "Adagrad": torch.optim.Adagrad,
    "RMSprop": torch.optim.RMSprop,
    "Adam": torch.optim.Adam
}

# 4️⃣ Train function
def train_model(opt_name, epochs=3):
    model = Net()
    optimizer = optimizers[opt_name](model.parameters(), lr=0.01)
    loss_fn = nn.CrossEntropyLoss()
    losses = []

    for epoch in range(epochs):
        total_loss = 0
        for x, y in train_loader:
            optimizer.zero_grad()
            out = model(x)
            loss = loss_fn(out, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        losses.append(total_loss / len(train_loader))
        print(f"{opt_name} - Epoch {epoch+1}, Loss: {losses[-1]:.4f}")

    correct = 0
    total = 0
    with torch.no_grad():
        for x, y in test_loader:
            out = model(x)
            _, pred = torch.max(out, 1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    acc = 100 * correct / total
    print(f"{opt_name} Accuracy: {acc:.2f}%\n")

    return losses, acc

# 5️⃣ Train and visualize
epochs = 5
results = {name: train_model(name, epochs) for name in optimizers}
losses = {k: v[0] for k, v in results.items()}
accuracies = {k: v[1] for k, v in results.items()}

plt.figure(figsize=(7,4))
for name, loss in losses.items():
    plt.plot(loss, label=name)
plt.xlabel("Epochs")
plt.ylabel("Training Loss")
plt.title("Comparison of Optimizers on MNIST")
plt.legend()
plt.show()
